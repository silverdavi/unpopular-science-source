\begin{SideNotePage}{
  \textbf{Top (Chinese Room Experiment):} The Chinese Room Thought Experiment (John Searle, 1980). An English-speaking person sits inside a sealed room, using a rulebook to manipulate Chinese symbols without understanding their meaning. The system produces fluent-looking Chinese responses purely through syntactic manipulation. Searle’s point: following formal symbol-manipulation rules does not constitute understanding. \par
  \textbf{Bottom (Modern Language Models (LLMs)):} Instead of explicit rules, LLMs use high-dimensional vector representations and learned statistical patterns from massive datasets. Inputs are converted into embeddings, processed through multiple nonlinear layers, and decoded into probable outputs. Yet, despite the architectural difference, LLMs — like the Chinese Room — operate without intrinsic semantic understanding. They produce contextually appropriate language based on training distributions, not genuine comprehension.
}{20_ChineseRoomArgument/20_ Capish, Comprende, Computes_.pdf}
\end{SideNotePage}

\begin{historical}
Alan Turing’s 1950 essay introduced the “imitation game” (later known as the Turing Test), proposing that if a machine’s textual responses were indistinguishable from a human’s, it could be considered intelligent. This marked the beginning of modern debates on machine cognition. In the following decades, the symbolic AI movement gained momentum, with figures like John McCarthy formalizing logic-based systems and Jerry Fodor proposing the “Language of Thought” hypothesis, which treated mental processes as manipulations of internal symbolic representations.

By the late 1970s, optimism surrounding symbolic approaches began to collide with deeper philosophical questions. Critics questioned whether syntactic manipulation alone could account for semantics — understanding rooted in meaning. In 1980, John Searle articulated the Chinese Room argument, asserting that executing formal rules does not entail comprehension. His critique challenged the assumption of strong AI: that implementing a program is equivalent to having a mind.

Contemporaneously, Daniel Dennett proposed the “intentional stance,” emphasizing observer-relative attributions of belief and intention, while Patricia and Paul Churchland advocated for eliminative materialism, arguing that folk-psychological terms like “belief” and “desire” might eventually be replaced by neurobiological accounts. Meanwhile, connectionist models — distributed neural networks — began gaining traction in the mid-1980s, offering an alternative to rule-based systems by emphasizing statistical learning over symbolic structure.

By the 1990s, AI had achieved public milestones such as Deep Blue’s 1997 victory over Garry Kasparov. Yet critics noted that performance alone does not imply understanding. With the advent of large language models in the 2010s and 2020s, capable of generating coherent and contextually appropriate text, the debate has re-emerged: do these systems understand language, or are they sophisticated instances of the Chinese Room, manipulating symbols without grasping their meaning? Public reports about recent frontier models (e.g., GPT-4) leave specific parameter counts and training token totals undisclosed, though they are trained at internet scale.
\end{historical}

Electric energy is often described as flowing through wires, like water through a pipe. This analogy is common but misleading. It treats energy as a substance carried by electrons moving from source to device. But electrons inside a conductor move slowly. Their average drift velocity under a typical voltage is only a few millimeters per second. What moves quickly is not the charge, but the disturbance in the electromagnetic field that propagates through space. A better analogy, if still inaccurate, is a wave traveling across the surface of a pond: the water does not move forward, but the wave does. In the same way, electric energy is transmitted by the wave-like interaction of fields, not by the displacement of material particles.

This effect is clear in static electricity: when materials are rubbed together, electrons transfer but no current flows, yet a strong electric field appears in surrounding space that can exert forces and store energy. Similarly, when a switch closes in a circuit, devices respond almost instantly because electromagnetic fields establish throughout the geometry simultaneously. A voltage sets up an electric field $\mathbf{E}$ along the wire, current produces a magnetic field $\mathbf{B}$ encircling it, and these fields extend beyond the conductor's surface, occupying surrounding space and determining energy's path.

This behavior is formalized in Maxwell's equations. Before Maxwell, electric and magnetic phenomena were treated separately: electric fields originated from static charges, and magnetic fields from moving charges, that is, from currents. These laws worked well for static situations but failed in time-varying regimes. Maxwell identified a critical gap in Ampère's law. According to its original form, a magnetic field was produced only by conduction current. But this led to contradictions in cases where the electric field changed in time but no actual current flowed, such as inside a capacitor during charging. Maxwell resolved the inconsistency by introducing the concept of displacement current: the idea that a changing electric field $\partial \mathbf{E}/\partial t$ acts like a current, generating a magnetic field even in the absence of moving charge.

This single correction was momentous. The displacement current term transformed a collection of separate electromagnetic laws into a mathematically consistent system of equations. This completed system contained a wave equation with a definite propagation speed: $c = 1/\sqrt{\varepsilon_0 \mu_0}$. When Maxwell calculated this speed using known electromagnetic constants, it matched the measured speed of light. This indicated that light itself was an electromagnetic phenomenon. However, this consistency exposed an incompatibility: Maxwell's equations predicted that light travels at the same speed in all reference frames, while Newton's mechanics (and common sense) required speeds to transform according to Galilean relativity — that if you move away from a light source at half the speed of light, you should see the light move away from you at half the speed. The consistency that Maxwell achieved revealed an incompatibility at the heart of classical physics — one that would not be resolved until Einstein's special relativity.

The completed equations describe how electric and magnetic fields sustain each other: a changing electric field $\mathbf{E}$ generates a magnetic field $\mathbf{B}$, and a changing $\mathbf{B}$ regenerates $\mathbf{E}$. This mutual coupling produces wave propagation even in the absence of charge or current, with perpendicular oscillating fields that constitute the electromagnetic energy. To compute energy flow, the magnetic field $\mathbf{B}$ must be normalized by the vacuum permeability: $\mathbf{H} = \mathbf{B} / \mu_0$. This ensures that both $\mathbf{E}$ and $\mathbf{H}$ are expressed in compatible units when evaluating energy flux. The Poynting vector, $\mathbf{S} = \mathbf{E} \times \mathbf{H}$, describes the instantaneous direction and intensity of electromagnetic energy transport. It has units of power per unit area (W/m$^2$) and is always perpendicular to both fields.

In high-energy conventions one often sets $c=1$ and $\varepsilon_0=\mu_0=1$, in which case $\mathbf{B}$ and $\mathbf{H}$ share units and coincide in vacuum.

Near a long straight wire, $\mathbf{B}$ encircles the wire azimuthally, while surface charges establish an $\mathbf{E}$ field with a component along the wire; in coaxial or two‑wire transmission lines, $\mathbf{E}$ is predominantly transverse (radial). In all cases, the Poynting vector $\mathbf{S}=\mathbf{E}\times\mathbf{H}$ points along the direction of power flow in the surrounding space, not within the conductor. The conductors establish boundary conditions that constrain and guide the fields; the energy transfer occurs in the fields occupying the space around the conductors.

Now for the role of electrons. Before electrons were understood as discrete particles, early models of electricity imagined it as a continuous substance flowing through wires, like water through a pipe. In this mechanical analogy, the conductor acted as a passive conduit, and the electric current was treated as an invisible, uniform fluid. The observable effects of voltage and current were attributed to the movement of this substance through the wire. While this model offered some intuition for the flow of charge, it could not account for material differences or thermal effects, lacking any microscopic description of matter that would allow calculation of materials' behavior under applied voltage.

In 1900, Paul Drude introduced a kinetic theory of conduction that treated electrons as classical particles moving freely between instantaneous collisions with heavy, stationary ions in a metallic lattice. Under an applied electric field, the electrons acquired a small net drift velocity superimposed on their thermal motion, giving rise to a steady current. This model successfully reproduced Ohm's law of resistance and introduced the concept of mean free path. However, it relied on Maxwell–Boltzmann statistics and treated electrons as distinguishable particles in thermal equilibrium. These assumptions, though reasonable for a dilute gas, led to contradictions when applied to dense electron systems in metals.

Multiple contradictions arose. Electronic heat capacity: classical theory predicted each electron would contribute $\tfrac{3}{2}k_B$ (where $k_B$ is the Boltzmann constant), but calorimetry showed values over a hundred times smaller, indicating most electrons couldn't gain thermal energy. The Wiedemann–Franz law: the ratio $L = \kappa / (\sigma T)$ should be constant ($\approx 2 \times 10^{-8}$ W·$\Omega$/K$^2$), but experiments showed temperature variation, implying different transport mechanisms. Electrostatic shielding: fields applied outside conductive enclosures weren't detected inside, but Drude's model offered no mechanism for this suppression since it treated electrons as isolated particles.

Further contradictions came from temperature-dependent resistivity. Drude's model predicted that resistivity should increase linearly with temperature due to more frequent electron–ion collisions. In practice, resistivity curves showed deviations from linearity, especially at low temperatures where resistance often plateaued or decreased. High-purity metals with large crystalline domains exhibited behavior that depended sensitively on defect density, lattice structure, and impurity concentration. These features played no role in Drude's theory, which treated the lattice as a uniform background. The observed dependence on details suggested that new scattering mechanisms and quantum restrictions were at play.

In 1912, Peter Debye addressed the failures of the Drude model by incorporating lattice dynamics into the theory of conduction. Instead of treating ions as fixed scattering centers, he modeled them as thermally vibrating masses whose motion becomes increasingly pronounced with temperature. These vibrations were treated as quantized normal modes — modernly termed phonons — which represent collective oscillations of the atomic lattice. Unlike localized particle collisions, phonons describe delocalized, wave-like excitations that span the crystal and interact coherently with conduction electrons. As the temperature rises, the number and amplitude of accessible phonon modes increase, leading to more frequent electron–phonon collisions and higher resistivity. This introduced a temperature-dependent scattering mechanism that aligned more closely with observed trends in metallic resistance.

The phonon model explained resistivity behavior: linear growth at high temperatures due to increased phonon population, and saturation at low temperatures where phonon modes freeze out. High-purity metals showed stronger temperature effects since electron–phonon scattering dominated over impurity scattering.

Debye also introduced electrostatic screening: conduction electrons collectively redistribute to cancel external fields within a characteristic Debye length (typically nanometers). This explained perfect shielding in conductors and redefined them as collectively responsive media.
In metals, electrostatic screening is governed by the dense electron gas and characterized by the Thomas–Fermi screening length (typically on the order of an ångström). The Debye length is appropriate for dilute plasmas and electrolytes; in conductors, free electrons rearrange to cancel external fields within this much shorter scale, producing near‑perfect shielding.

While Debye focused on the quantized behavior of the lattice, in 1928, Arnold Sommerfeld turned to the electron gas itself. Debye's model resolved key thermal anomalies by treating lattice vibrations as phonons and introducing collective screening, but it still relied on classical statistics for the electrons. Sommerfeld's contribution was to replace the classical electron gas with a quantum one, governed by the Pauli exclusion principle. In this revised model, electrons occupy discrete quantum states and fill all available levels up to the Fermi energy (the highest occupied level at absolute zero). Only those near this surface can change state when a weak external field is applied. This restriction explains why most electrons do not contribute to conduction or heat capacity, despite their large individual velocities. It also accounts for the small but nonzero electronic heat capacity and the weak temperature dependence of conductivity in pure metals. Sommerfeld's approach completed the redefinition of conduction: not as thermal drift through a static lattice, but as the quantum response of a filled electron sea to external perturbation.

The Sommerfeld model resolved the longstanding discrepancy in the electronic heat capacity. Classical theories assumed that all conduction electrons share thermal energy, leading to a heat capacity proportional to temperature and electron count. Measurements showed a smaller contribution, growing linearly with temperature but with a suppressed coefficient. Sommerfeld explained this through quantum mechanics: only electrons within a narrow energy window around the Fermi level can absorb energy and transition to higher states. The rest are blocked by the exclusion principle. This result matched calorimetric data and clarified why the electronic contribution vanishes at low temperature, while the lattice contribution remains governed by phonon dynamics.

The same explanation also accounts for the weak temperature dependence of conductivity. Because only a small fraction of electrons near the Fermi surface can shift momentum under an applied field, the number of active carriers remains nearly constant as temperature changes. Scattering rates still vary — especially due to phonons — but the carrier population does not. This also improved the theoretical form of the Wiedemann–Franz law. By combining quantum statistics for the electron gas with Debye's treatment of the lattice, the temperature scaling of both thermal and electrical conductivity was derived with the correct proportionality constant. Sommerfeld's model provided a foundation for the thermal and electrical behavior of metals across temperature regimes.

Although each electron near the Fermi surface contributes to conduction, the resulting motion is slow. The net velocity acquired from an applied electric field is called the drift velocity. It is given by $v_d = I / (n A e)$, where $I$ is the current, $n$ is the charge carrier density, $A$ is the cross-sectional area of the conductor, and $e$ is the elementary charge. For typical metals carrying macroscopic currents, this drift speed is on the order of a fraction of a millimeter per second. Despite the vast number of electrons involved, their collective motion results in a current that builds slowly and transports charge gradually along the wire. The slowness of this process is an outcome of the Fermi-level restriction and the small imbalance imposed by weak electric fields.

At the macroscopic level, conductors don't carry energy — they impose boundary conditions on electromagnetic fields. Free charge ensures the electric field $\mathbf{E}$ vanishes inside conductors, shaping fields outside and fixing their orientation. Current sets the magnetic field $\mathbf{B}$ in surrounding space, with wire geometry anchoring the field configuration. This role becomes explicit in guided-wave systems: waveguides and coaxial cables confine fields by geometry, allowing energy to flow through space between conductors as modes determined by Maxwell's equations. In contrast, radiative systems like antennas lack boundaries, so fields spread outward and energy disperses.

\begin{commentary}[Energy Beyond the Wire: The Veritasium Debate]
A popular Veritasium video — from the science education YouTube channel known for physics demonstrations — brought renewed attention to the role of the Poynting vector in electrical circuits. It correctly emphasized that electromagnetic energy flows through the space surrounding the wires, not within them. However, the video also presented a misleading claim: that a lightbulb placed one meter away from a battery through air would begin to light in approximately $1 m/c$ seconds, as if energy propagated directly through the air across that gap.

The setup featured a wire several meters long stretched across a field, with the bulb positioned one meter from the battery in physical space. But electrically, the bulb was located many meters away, along the entire length of the conductor. The suggestion that energy could traverse the air and reach the bulb independently of the wiring contradicted the actual role of the circuit.

Electromagnetic energy is shaped and guided by the fields surrounding the conductors. While the energy resides in the space outside the wire, its direction of flow — represented by the Poynting vector — follows the current path determined by the circuit's geometry. The dominant flow of power reaches the bulb by propagating along this field, not by crossing the shortest spatial distance. For a resistive load to receive power, it must participate in a closed conductive path that supports both current and field continuity. A filament bulb does not extract energy from the ambient field; it is not designed to behave as a receiving antenna.

The video's central claim — that energy flows outside the wire — is conceptually correct. But its example confuses geometric proximity with electromagnetic coupling. Energy does not flow from source to load along the shortest physical line. It follows the field configuration established by the conductors, which define the actual connectivity of the circuit.
\end{commentary}
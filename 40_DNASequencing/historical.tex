\begin{historical}
Frederick Sanger's 1977 chain-termination method emerged from years of frustration with earlier approaches to reading DNA. His insight — using modified nucleotides to randomly terminate DNA synthesis — provided the first practical way to determine base sequences. While Allan Maxam and Walter Gilbert simultaneously developed a chemical cleavage method, Sanger's approach proved more robust and became the foundation for three decades of genomic research.

The Human Genome Project launched in 1990 as biology's moonshot — a publicly funded effort to read all 3.2 billion letters of human DNA. Francis Collins led the international consortium, methodically mapping and sequencing chromosomes piece by piece. Then in 1998, Craig Venter announced that his company, Celera Genomics, would sequence the human genome in just three years using a “whole genome shotgun” approach — fragmenting the entire genome at once and using computational power to reassemble it.

The race was on. The public project, with its careful clone-by-clone strategy, suddenly faced a nimble competitor unconstrained by academic collaboration requirements. Venter's team used hundreds of automated sequencers running 24/7, while the public consortium scrambled to accelerate their timeline. Both sides published draft sequences simultaneously in February 2001 — a diplomatic resolution to a bitter competition that had featured Congressional hearings, patent disputes, and public acrimony. The project cost approximately \$3 billion and required a decade of work.

Sanger sequencing's limitations — high cost and low throughput — motivated a new generation of technologies. 454 Life Sciences introduced pyrosequencing in 2005, detecting DNA synthesis through light emission and reading millions of fragments simultaneously. This began the “next-generation” era, where parallelization replaced precision.

Illumina emerged as the dominant platform after acquiring Solexa technology in 2007. Their reversible terminator chemistry solved pyrosequencing's homopolymer problems while maintaining massive throughput. The cost per genome plummeted from millions to thousands of dollars, democratizing genomic research.

The push for longer reads drove development of single-molecule technologies. Pacific Biosciences spent a decade perfecting zero-mode waveguides — nanoscale observation chambers that could watch individual DNA polymerase enzymes at work. Oxford Nanopore took a different path, threading DNA through protein pores and reading the sequence from electrical current fluctuations. When they released the MinION in 2014 — a sequencer the size of a USB stick — it showcased the progress the technology has made from room-sized machines of the genome project era.

The computational challenge evolved in parallel. Early assembly algorithms handled thousands of Sanger reads; modern de Bruijn graph methods process billions of short reads. Long-read assemblers now tackle the ultimate challenge: reconstructing complete chromosomes from end to end. Sequencing costs have fallen faster than Moore's Law — from roughly dollars per base in 1990 to well under a dollar per megabase today, and under a thousand dollars per human genome on leading platforms.

\end{historical}
